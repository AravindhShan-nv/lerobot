# GR00T N1.6 Policy

GR00T N1.6 is the latest version of NVIDIA's open foundation model for humanoid robot reasoning and skills. It introduces several improvements over N1.5 including AlternateVLDiT architecture, 32 DiT layers (vs 16), unfrozen top 4 VLM layers, and state-relative action chunks.

## Installation Requirements

GR00T N1.6 requires flash attention and several additional dependencies. Follow these steps to set up your environment:

### Step 1: Create Conda Environment

```bash
source ~/miniforge3/etc/profile.d/conda.sh
conda create -n lerobot-groot python=3.10 -y
conda activate lerobot-groot
```

### Step 2: Install System Dependencies

```bash
conda install ffmpeg -c conda-forge
```

### Step 3: Install PyTorch and Flash Attention

```bash
# Check https://pytorch.org/get-started/locally/ for your system
uv pip install "torch>=2.2.1,<2.8.0" "torchvision>=0.21.0,<0.23.0"  # --index-url https://download.pytorch.org/whl/cu1XX

# Flash attention dependencies
uv pip install ninja "packaging>=24.2,<26.0"

# Install flash attention (requires CUDA)
uv pip install "flash-attn>=2.5.9,<3.0.0" --no-build-isolation

# Verify installation
python -c "import flash_attn; print(f'Flash Attention {flash_attn.__version__} imported successfully')"
```

### Step 4: Install LeRobot with Groot Dependencies

```bash
cd /path/to/lerobot
uv pip install -e ".[groot]"
```

### Step 5: Install Additional N1.6 Dependencies

N1.6 requires some additional packages not included in the base `[groot]` dependencies:

```bash
uv pip install lmdb==1.7.5
uv pip install albumentations==1.4.18
uv pip install "faker>=33.0.0,<35.0.0"
uv pip install tyro
uv pip install "matplotlib>=3.10.3,<4.0.0"
```

### Step 6: Install Development/Test Dependencies (Optional)

```bash
uv pip install pytest
uv pip install lark
```

## Usage

To use GR00T N1.6 in your LeRobot configuration, specify the policy type as:

```python
policy.type=gr00t_n1d6
```

## Training

Training run tracker (Google Sheet): [GR00T N1.6 Training Runs](https://docs.google.com/spreadsheets/d/1-L6RSxmxhd96hG2YynV5GBYTxHxhmB6rdsW7U4lnKaI/edit?gid=0#gid=0)

### Training Command Example

#### Libero Train

```bash
DATASET_REPO_ID=HuggingFaceVLA/libero  # change if training on a different dataset
OUTPUT_DIR=/ephemeral/outputs/train/gr00t_n1d6_libero_object_v2_20k  # change output path
POLICY_REPO_ID=aravindhs-NV/libero_test_debug_inference_0208_v2_20k  # change HF repo for checkpoints

nohup lerobot-train \
    --policy.type=gr00t_n1d6 \
    --dataset.repo_id=$DATASET_REPO_ID \
    --policy.embodiment_tag=libero_panda \
    --policy.use_relative_action=false \
    --batch_size=64 \
    --steps=20000 \
    --log_freq=100 \
    --save_freq=10000 \
    --output_dir=$OUTPUT_DIR \
    --policy.repo_id=$POLICY_REPO_ID \
    > libero_train_v2_20k.log 2>&1 &
```

#### SO100 Train

Very important: set `--dataset.revision=main` for this dataset specifically (not needed for other datasets). The default `3.0` branch contains an extra noisy episode.

```bash
DATASET_REPO_ID=sreetz-nv/so101_teleop_vials_rack_left_real_50  # change dataset if needed
DATASET_REVISION=main  # keep as main for this dataset (important)
OUTPUT_DIR=/ephemeral/outputs/train/gr00t_n1d6_fixed_stats_v1  # change output path
POLICY_REPO_ID=aravindh-NV/lerobot_grootn16_debug_inference_0208_v1  # change HF repo for checkpoints

nohup lerobot-train \
    --policy.type=gr00t_n1d6 \
    --dataset.repo_id=$DATASET_REPO_ID \
    --dataset.revision=$DATASET_REVISION \
    --batch_size=32 \
    --steps=10000 \
    --log_freq=100 \
    --save_freq=5000 \
    --output_dir=$OUTPUT_DIR \
    --policy.repo_id=$POLICY_REPO_ID \
    > so100_train_10k.log 2>&1 &
```

### Multi-GPU Training

#### SO100 Multi-GPU Train (8 GPUs)

Note: effective global batch size is `batch_size * num_processes` (here: `16 * 8 = 128`). Reduce `batch_size` or `num_processes` if you hit OOM.

```bash
NUM_PROCESSES=8  # change based on number of GPUs
DATASET_REPO_ID=sreetz-nv/so101_teleop_vials_rack_left_real_50  # change dataset if needed
DATASET_REVISION=main  # keep as main for this dataset (important)
OUTPUT_DIR=/ephemeral/outputs/train/gr00t_n1d6_balanced_bs16_steps10k  # change output path
POLICY_REPO_ID=aravindh-NV/lerobot_grootn16_balanced_bs16_steps10k  # change HF repo for checkpoints
LOG_FILE=lerobot_grootn16_balanced_bs16_steps10k_v3.log  # change log filename if needed

nohup accelerate launch \
  --multi_gpu \
  --num_processes=$NUM_PROCESSES \
  --mixed_precision=bf16 \
  $(which lerobot-train) \
  --policy.type=gr00t_n1d6 \
  --dataset.repo_id=$DATASET_REPO_ID \
  --dataset.revision=$DATASET_REVISION \
  --batch_size=16 \
  --steps=10000 \
  --log_freq=100 \
  --save_freq=5000 \
  --output_dir=$OUTPUT_DIR \
  --policy.repo_id=$POLICY_REPO_ID \
  > $LOG_FILE 2>&1 &
```

## Running Tests

### N1.6 Tests

```bash
cd /path/to/lerobot
source ~/anaconda3/etc/profile.d/conda.sh
conda activate lerobot-groot
pytest tests/policies/groot/test_gr00t_n1d6_lerobot.py -v --ignore=tests/policies/groot/test_groot_vs_original.py
```

### Open loop evaluation

#### SO100 Open Loop Eval

Note: open-loop evaluation is currently only working for relative-action setups.

```bash
POLICY_REPO_ID=aravindhs-NV/gr00t_n1d6_fixed_stats_0208  # change to your policy repo
DATASET_REPO_ID=sreetz-nv/so101_teleop_vials_rack_left_real_50  # change dataset if needed
SAVE_DIR=open_loop_eval_debug_0208  # change output folder

python src/lerobot/scripts/open_loop_eval_v4.py \
    --policy-repo-id=$POLICY_REPO_ID \
    --dataset-repo-id=$DATASET_REPO_ID \
    --episode_ids=0 1 2 3 4 \
    --save-dir=$SAVE_DIR \
    --device=cuda \
    --inference-interval=16
```

### Libero evaluation

```bash
POLICY_PATH=aravindhs-NV/libero_test_debug_inference_0208_v2_20k  # change to your policy path/repo
OUTPUT_DIR=eval_libero_20k_0209  # change eval output folder

lerobot-eval \
    --policy.path=$POLICY_PATH \
    --env.type=libero \
    --env.task=libero_object \
    --eval.batch_size=1 \
    --eval.n_episodes=10 \
    --output_dir=$OUTPUT_DIR \
    2>&1 | tee eval_libero_20k.log
```

### SO100 Real Robot Eval

```bash
TELEOP_PORT=/dev/ttyUSB0  # change leader serial port
TELEOP_ID=leader_arm  # change leader arm ID
ROBOT_PORT=/dev/ttyUSB1  # change follower serial port
ROBOT_ID=follower_arm  # change follower arm ID
CAMERAS_JSON='{"wrist": {"type": "opencv", "index_or_path": 4, "width": 640, "height": 480, "fps": 30}, "front": {"type": "opencv", "index_or_path": 6, "width": 640, "height": 480, "fps": 30}}'  # update camera indices/paths
TASK_TEXT="Pick up the vial and place it in the yellow rack"  # update for your task
DATASET_REPO_ID=sreetz-nv/eval_groot-lerobot-debug-0211-bs16-10k  # change eval dataset repo
POLICY_PATH=aravindhs-NV/groot-lerobot-debug-0211-bs16-10k  # change policy checkpoint/repo

lerobot-record \
  --teleop.type=so101_leader \
  --teleop.port=$TELEOP_PORT \
  --teleop.id=$TELEOP_ID \
  --robot.type=so101_follower \
  --robot.port=$ROBOT_PORT \
  --robot.id=$ROBOT_ID \
  --robot.cameras="$CAMERAS_JSON" \
  --dataset.single_task="$TASK_TEXT" \
  --dataset.repo_id=$DATASET_REPO_ID \
  --dataset.episode_time_s=30 \
  --dataset.num_episodes=3 \
  --policy.path=$POLICY_PATH \
  --policy.n_action_steps=16 \
  --dataset.push_to_hub=true
```

## Comparing N1.6 LeRobot v/s Original GR00T

```bash
cd ~/gr00t_lerobot/lerobot && source ~/anaconda3/etc/profile.d/conda.sh && conda activate lerobot-groot
pytest tests/policies/groot/test_gr00t_n1d6_standalone.py -v -s


pytest tests/policies/groot/test_gr00t_n1d6_standalone.py -v --ignore=tests/policies/groot/test_groot_vs_original.py

cd ~/gr00t_lerobot/Isaac-GR00T && source ~/anaconda3/etc/profile.d/conda.sh && conda activate isaac-groot
python tests/test_gr00t_n1d6_standalone.py


cd ~/gr00t_lerobot/Isaac-GR00T && source ~/miniforge3/etc/profile.d/conda.sh && conda activate groot16 && uv run python tests/test_gr00t_n1d6_standalone.py
```

### N1.5 Tests

```bash
pytest tests/policies/groot/test_groot_lerobot.py -v --ignore=tests/policies/groot/test_groot_vs_original.py
```

## Key Differences from N1.5

| Feature        | N1.5              | N1.6                         |
| -------------- | ----------------- | ---------------------------- |
| DiT Layers     | 16                | 32                           |
| DiT Type       | DiT               | AlternateVLDiT               |
| VLM Adapter    | 4-layer post-VLM  | Unfrozen top 4 VLM layers    |
| State Encoder  | Linear projection | CategorySpecificMLP          |
| Action Encoder | Simple embedding  | MultiEmbodimentActionEncoder |
| Action Chunks  | Absolute          | State-relative               |
| Eagle Backbone | Eagle 2.5         | Eagle 3 (Block2A-2B-v2)      |
| Action Horizon | 50                | 40                           |

## License

This model follows the **Apache 2.0 License**, consistent with the original [GR00T repository](https://github.com/NVIDIA/Isaac-GR00T).

## Resume

```bash
source ~/miniforge3/etc/profile.d/conda.sh
conda activate lerobot-groot

export WANDB_PROJECT=
export WANDB_ENTITY=


DATASET_NAME=finish_sandwich
DATASET_REPO=izuluaga/finish_sandwich

POLICY_ROOT_HF=nvkartik
POLICY_TYPE=gr00t_n1d6
HUB_REPO="${POLICY_ROOT_HF}/${POLICY_TYPE}-${DATASET_NAME}-2bs-resume"

# Training parameters
BATCH_SIZE=2
STEPS=15000
SAVE_FREQ=5000
LOG_FREQ=50
WANDB_ENABLE=false
PUSH_TO_HUB=true

# Logging parameters
JOB_NAME="${POLICY_TYPE}-${DATASET_NAME}"
OUTPUT_DIR="outputs/train/${JOB_NAME}"

# accelerate training
lerobot-train \
    --dataset.repo_id="${DATASET_REPO}" \
    --batch_size="${BATCH_SIZE}" \
    --steps="${STEPS}" \
    --output_dir="${OUTPUT_DIR}" \
    --job_name="${JOB_NAME}" \
    --policy.device="cuda" \
    --wandb.enable="${WANDB_ENABLE}" \
    --policy.push_to_hub="${PUSH_TO_HUB}" \
    --policy.repo_id="${HUB_REPO}" \
    --save_freq="${SAVE_FREQ}" \
    --log_freq="${LOG_FREQ}" \
    --policy.tune_diffusion_model=false \
    --resume=true \
    --config_path=./outputs/train/gr00t_n1d6-finish_sandwich/checkpoints/last/pretrained_model/train_config.json
```
